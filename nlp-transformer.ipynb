{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_setup2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQgS8c40iJA1"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp7I3Qr0ntDw",
        "outputId": "ead81b7a-bb80-4874-c501-7fa1640af2c2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFa79OW-tVEF",
        "outputId": "b4e04288-3197-4062-c741-45456f0f36d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QZsxTYrkz_2"
      },
      "source": [
        "def tokenize_words(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "\n",
        "  filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "  return \" \".join(filtered)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_xGpZunmTuR"
      },
      "source": [
        "#file = open(\"/content/drive/MyDrive/audio_dataset/tr/data.txt\").read()\n",
        "\n",
        "tokens =  open(\"/content/drive/MyDrive/audio_dataset/tr/tokens.txt\").read()#tokenize_words(file)\n",
        "\n",
        "characters = sorted(list(set(tokens)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X96beUocziYt"
      },
      "source": [
        "char_to_num = dict((c, i) for i, c in enumerate(characters))\n",
        "num_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBxpnFTE0kgD"
      },
      "source": [
        "input_len = len(tokens)\n",
        "vocab_len = len(characters)\n",
        "\n",
        "seq_len = 100\n",
        "\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4iTAFGN1kIs"
      },
      "source": [
        "for i in range(0, (input_len) - seq_len, 1):\n",
        "    in_seq = tokens[i:i + seq_len]\n",
        "\n",
        "    out_seq = tokens[i + seq_len]\n",
        "\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClN-Lvyh3bxR",
        "outputId": "a2f520be-3246-4dc6-abbf-1bc1b3bfc52b"
      },
      "source": [
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 4044291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PpSHUI37onz"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_len, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ7vK_QvTAzY"
      },
      "source": [
        "numpy.save(\"codes.npy\", X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M6wMn57AMYI"
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjhPE4pdAQQu"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb99CU6pAywI"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFCdausjA0JN"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1osPEVA6O8",
        "outputId": "2e15211e-beb5-4918-d157-fc0b41692f7b"
      },
      "source": [
        "model.fit(X, y, epochs=30, batch_size=256, callbacks=desired_callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15799/15799 [==============================] - 1149s 71ms/step - loss: 2.3983\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.11632, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/30\n",
            "15799/15799 [==============================] - 1121s 71ms/step - loss: 1.7998\n",
            "\n",
            "Epoch 00002: loss improved from 2.11632 to 1.76664, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/30\n",
            "15799/15799 [==============================] - 1119s 71ms/step - loss: 1.6912\n",
            "\n",
            "Epoch 00003: loss improved from 1.76664 to 1.67429, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/30\n",
            "15799/15799 [==============================] - 1120s 71ms/step - loss: 1.6334\n",
            "\n",
            "Epoch 00004: loss improved from 1.67429 to 1.62388, saving model to model_weights_saved.hdf5\n",
            "Epoch 5/30\n",
            "15799/15799 [==============================] - 1122s 71ms/step - loss: 1.5964\n",
            "\n",
            "Epoch 00005: loss improved from 1.62388 to 1.59043, saving model to model_weights_saved.hdf5\n",
            "Epoch 6/30\n",
            "15799/15799 [==============================] - 1121s 71ms/step - loss: 1.5716\n",
            "\n",
            "Epoch 00006: loss improved from 1.59043 to 1.56664, saving model to model_weights_saved.hdf5\n",
            "Epoch 7/30\n",
            "15799/15799 [==============================] - 1121s 71ms/step - loss: 1.5520\n",
            "\n",
            "Epoch 00007: loss improved from 1.56664 to 1.54762, saving model to model_weights_saved.hdf5\n",
            "Epoch 8/30\n",
            "15799/15799 [==============================] - 1121s 71ms/step - loss: 1.5352\n",
            "\n",
            "Epoch 00008: loss improved from 1.54762 to 1.53227, saving model to model_weights_saved.hdf5\n",
            "Epoch 9/30\n",
            "15799/15799 [==============================] - 1122s 71ms/step - loss: 1.5243\n",
            "\n",
            "Epoch 00009: loss improved from 1.53227 to 1.51992, saving model to model_weights_saved.hdf5\n",
            "Epoch 10/30\n",
            "15799/15799 [==============================] - 1122s 71ms/step - loss: 1.5103\n",
            "\n",
            "Epoch 00010: loss improved from 1.51992 to 1.50899, saving model to model_weights_saved.hdf5\n",
            "Epoch 11/30\n",
            " 4128/15799 [======>.......................] - ETA: 13:48 - loss: 1.5023"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Oli7TPN6lj",
        "outputId": "83071ece-e6cd-424c-e702-b2b0c6744966"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\" ferent guys avalon sings song song heart cute female backing vocals violins piano feature maybe pian \"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}